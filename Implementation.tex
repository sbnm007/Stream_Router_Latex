\chapter{Implementation}

\section{Introduction}
The implementation chapter translates the proposed WebTransport/QUIC streaming architecture in Chapter 4 into a fully working, reproducible system. Firstly, it prepares the local development environment by installing Minikube \cite{minikube-docs} with its Docker driver on Linux Mint, enabling MetalLB \cite{metallb-docs} for external Load-Balancer Internet Protocol (IP) addresses, and configuring Wireshark \cite{wireshark-docs} together with self-signed Transport Layer Security (TLS) certificates so that encrypted QUIC traffic can be captured and inspected. A Domain Name System (DNS) entry in /etc/hosts binds the synthetic domain quic-aioquic.com to the clusterâ€™s MetalLB range, ensuring that both browser clients and backend services resolve the same address. With the groundwork in place, the chapter then progresses through three tightly coupled tiers: firstly, a browser-based WebTransport client that captures video, audio, chat, file and screen-share data, packetises each stream, and transmits them over independent unidirectional QUIC streams; then, a Python router built on aioquic \cite{aioquic-repo} that receives these streams, parses the custom 32-byte packet header, and routes payloads to backend microservices via hot-reloadable Yet Another Markup Language (YAML) rules; and finally, a set of generic, YAML-configured microservices that expose HyperText Transfer Protocol (HTTP) endpoints, optionally forward data to Apache Pulsar, and run inside Minikube as scalable Deployments. The entire stack (router, microservices, and Pulsar) is then containerised and deployed through Kubernetes manifests that include Secrets for certificates, ConfigMaps for live configuration, and LoadBalancer Services for external reachability, leveraging Apache Pulsar \cite{pulsar-helm-repo} for message streaming.



% Describing Minikube installation process
\section{Installation of Minikube}
The host operating system of the machine at work is Linux Mint, a Debian-based operating system. The steps below outline the installation steps, ensuring a working setup for running a local Kubernetes cluster.

% Providing installation commands
To install Minikube the following commands are executed in the terminal:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
\end{lstlisting}

After running these commands Minikube is up and running successfully. To make sure that the installation is working, run Minikube by typing the following:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
minikube start
\end{lstlisting}

% Explaining Minikube's default behavior
Note that the \texttt{minikube start} command uses the Docker driver by default. Minikube runs within a Docker container, which provides logical separation from the host system. This containerized approach ensures efficient resource utilization and isolation from the host environment.

% Including figure for CPU and memory usage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/minikube-cpunmem.png}
    \caption{Minikube CPU and Memory Usage}
    \label{fig:minikube-cpu-and-memory}
\end{figure}

Since Minikube operates within a container, it leverages the host operating system's kernel and configuration by default. This setup allows the Kubernetes cluster to access additional resources from the host, optimizing performance and scalability for local development and testing.

% Describing kubectl installation
It is important to note that \texttt{kubectl}, the Kubernetes command-line tool, is automatically installed alongside Minikube. To confirm the pods are running in the cluster, run the following command:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
kubectl get pods
\end{lstlisting}

% Including figure for kubectl presence
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/pods.png}
    \caption{Verification of Kubectl Presence}
    \label{fig:kgetpods}
\end{figure}

% Confirming successful installation
The Minikube cluster and Kubernetes have been successfully installed and configured on the system. This setup provides a fully working local Kubernetes setup which is ready for deploying and managing applications.

% Describing MetalLB configuration process
\section{Configuration of MetalLB}
MetalLB is a load balancer that operates at Layer 4, which is essential for exposing all services of type LoadBalancer in our Kubernetes cluster. Minikube provides a straightforward installation process for MetalLB through its addons feature.

% Listing available addons
To see the existing addons in Minikube, run the following command:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
minikube addons list
\end{lstlisting}

This command lists several addons provided by Minikube. Among them, you will notice that MetalLB is disabled by default. To enable it, the following command is executed:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
minikube addons enable metallb
\end{lstlisting}

% Explaining IP range configuration for MetalLB
MetalLB requires a range of IP addresses to assign to services created within the Kubernetes cluster. When the Minikube container was started with the \texttt{minikube start} command, it created a network interface in the background for the Minikube (i.e., Kubernetes) environment. This can be verified by running:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
ifconfig
\end{lstlisting}

For the machine in use, the network interface was named \texttt{br-5d..} and had the following values: \texttt{inet 192.168.49.1}, \texttt{netmask 255.255.255.0}, \texttt{broadcast 192.168.49.255}. This indicates that the last octet is open from 0 to 255 which creates a subnet that can be represented by \texttt{192.168.49.0/24}.

% Configuring MetalLB IP range
To configure MetalLB with appropriate IP address ranges, run the following command:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
minikube addons configure metallb
\end{lstlisting}

During configuration, it would prompt asking to enter the start and end IP addresses for the load balancer:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
Enter Load Balancer Start IP: 192.168.49.1
Enter Load Balancer End IP: 192.168.49.250
\end{lstlisting}

This configures the MetalLB load balancer to expose applications within the specified IP address range, enabling it to assign IPs to LoadBalancer-type services in the Kubernetes cluster.


% Describing Wireshark installation and usage process
\section{Network Traffic Capture}
To install Wireshark and successfully capture network traffic on Linux Mint OS, the following process was followed.

% Providing installation commands
\subsection{Run Commands}
First, install Wireshark using the package manager by executing the following command:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
sudo apt install wireshark
\end{lstlisting}

Launch Wireshark by running:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
wireshark
\end{lstlisting}

Above command opens up the Wireshark application. Upon launching, Wireshark prompts you to select a specific network interface to monitor. Once an interface is selected, Wireshark begins capturing real-time traffic to and from the chosen network interface.



% Describing Chromium TLS key logging process
\subsection{Logging Chromium Client TLS Keys}
To log the TLS session keys for the Chromium client, the following steps were performed to ensure proper configuration and logging of the keys for use in decrypting network traffic.

% Providing commands for logging TLS keys
First, terminate any running instances of Chromium by executing the following command:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
pkill chromium
\end{lstlisting}

Next, set the environment variable to specify the path for the TLS key log file:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
export SSLKEYLOGFILE=$HOME/premaster-secret/sslkeylogfile.log
\end{lstlisting}

Then, restart Chromium to begin logging the TLS keys to the specified path.

% Including figure for SSL key log
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/sslkeylog.png}
    \caption{Chromium Client's SSL Key Log}
    \label{fig:chromiumssl}
\end{figure}

\subsection{Configuring TLS Keys in Wireshark}

This process successfully logs the TLS session keys to the designated file, enabling the decryption of TLS-encrypted traffic captured from the Chromium client.

% Explaining limitations and TLS decryption
It is important to note that higher-level application protocols, which are encrypted using the standard TLS protocol, can be captured but cannot be inspected directly due to their encryption. To decrypt such communications, TLS session keys are required. These session keys can be logged from the client or server and loaded into Wireshark for decryption.

% Configuring Wireshark for TLS decryption
To decrypt TLS traffic, TLS session keys must be loaded and to load the keys, following steps in Wireshark were followed:

\begin{enumerate}
    \item Go to \texttt{Edit -> Preferences -> Protocols -> TLS}.
    \item In the TLS settings, locate the \texttt{Pre-Master Secret Log Filename} field.
    \item Load the TLS session keys by adding the path of the log file which contain the keys.
\end{enumerate}

This process enables Wireshark to decrypt the TLS-encrypted data captured over the network, allowing for detailed inspection of the communication.




% Describing certificate management process
\section{Certificate Management}
To enable secure communication with the state-of-the-art WebTransport Streams HTTP/3 Router, it was determined that a certificate is essential, as data transmission over the specified protocol requires encryption. Since acquiring a domain was not feasible, the approach of creating a self-signed certificate was adopted.

% Providing commands for creating a self-signed certificate
The following command was used to generate a self-signed certificate for a custom domain:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
openssl req -x509 -newkey rsa:4096 -keyout tls.key -out tls.crt -days 365 -nodes -subj "/CN=quic-aioquic.com"
\end{lstlisting}

This command instructs OpenSSL to create an RSA key pair, generating both a certificate (\texttt{tls.crt}) and a private key (\texttt{tls.key}) with an expiration period of 365 days. The \texttt{-nodes} flag ensures the private key is not encrypted, and the Subject Alternative Name and Common Name (CN) are set to the custom domain \texttt{quic-aioquic.com}.

% Explaining certificate usage and client configuration
To utilize the certificate, the client must forward requests to the specified domain, and the client itself must trust the domain. Since the client in use was the Chrome browser, the self-signed certificate was imported through the browser's settings to establish trust.

% Providing steps for importing the certificate in Chrome
The certificate can be imported by following these steps in Chrome:

\begin{enumerate}
    \item Navigate to \texttt{Settings -> Privacy and Security -> Manage Certificates}.
    \item Under the \texttt{Installed Certificates by You} section, select \texttt{Import}.
    \item Follow the prompts to upload the \texttt{tls.crt} certificate file.
\end{enumerate}

This process ensures that the Chrome client trusts the self-signed certificate, enabling secure communication with the WebTransport Streams HTTP/3 Router.


% Describing DNS entry configuration process
\section{DNS Entry Configuration}
To enable domain name resolution for the local setup, a DNS entry must be configured. With the certificates properly structured, requests are explicitly checked for the specified domain. On a Linux machine, this configuration is achieved by modifying the \texttt{/etc/hosts} file to include a DNS entry that maps the domain to a specific IP address.

% Providing the DNS entry configuration
For the system in use, the following line was added to the \texttt{/etc/hosts} file:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
192.168.49.201 quic-aioquic.com
\end{lstlisting}

This entry maps the domain name \texttt{quic-aioquic.com} to the IP address \texttt{192.168.49.201}, ensuring proper domain name resolution for the local setup.




% Describing the client construction process
\section{Client Construction}
The construction of the client was accomplished using several core components, each designed to ensure a robust and user-friendly streaming experience. These components are detailed below.

% Describing the user interface design
\subsection{User Interface Design}
The client's user interface is created for clarity and functionality, offering an intuitive overview of the streaming process. The layout is structured using a responsive \textbf{CSS Grid} (\texttt{.container}), which divides the main content into distinct panels for streamlined user interaction.

\begin{itemize}
    \item \textbf{Video \& Audio Stream Panel} (\texttt{.panel}): This panel prominently displays the local video feed (\texttt{<video id="localVideo">}) and includes controls for initiating or stopping streaming, toggling video quality, and starting or stopping optional file and screen data streams.
    \item \textbf{Live Chat Panel} (\texttt{.panel}): This features a scrollable message display area (\texttt{.chat-\allowbreak{}messages}) and an input field with a send button (\texttt{.chat-input}) for real-time text communication.
    \item \textbf{Metrics Display} (\texttt{.metrics}): This provides at-a-glance performance indicators, including the count of video frames, audio packets, chat messages sent, and the calculated data transfer rate.
    \item \textbf{Status Bar} (\texttt{\#status}): This offers immediate feedback on the WebTransport connection state.
    \item \textbf{System Logs Panel} (\texttt{.logs}): This acts as a console, displaying critical information about stream creation, packet transmission, and error events, which is essential for monitoring and debugging during development and testing.
\end{itemize}

The visual design adopts a dark theme using CSS, enhancing readability in low-light conditions and providing a modern aesthetic. Buttons are styled with distinct colors (\texttt{.btn-start}, \texttt{.btn-stop}, \texttt{.btn-toggle}) to clearly indicate their actions and current states, such as \texttt{disabled} styling.

% Describing WebTransport connection setup
\subsection{WebTransport Connection Setup}
Establishing a reliable connection to the WebTransport server is the foundational step for all streaming operations. This is managed within the \texttt{WebTransportStreamer} class's \texttt{startStreaming()} method. The connection is initiated by creating a new \texttt{WebTransport} instance, specifying the server URL (\texttt{https://quic-aioquic.com:4433/wt}), which points to the secure QUIC endpoint of the server.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
//Initialize WebTransport connection to the server endpoint
this.transport = new WebTransport(this.serverUrl);
\end{lstlisting}

To ensure robust operation, event listeners are set up for \textbf{connection state changes} (\texttt{this.transport.onstatechange}) and \textbf{errors} (\texttt{this.transport.onerror}). These listeners are critical for monitoring the connection's health:

\begin{itemize}
    \item \texttt{onstatechange}: Updates the UI status and triggers \texttt{stopStreaming()} if the connection \texttt{state} transitions to \texttt{'closed'}, indicating a graceful shutdown or an unexpected termination.
    \item \texttt{onerror}: Catches any underlying transport errors, logs them to the console, and initiates \texttt{stopStreaming()} to clean up resources.
\end{itemize}

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
this.transport.onstatechange = () => {
    this.log(`Transport state: ${this.transport.state}`, 'info');
    if (this.transport.state === 'closed') {
        this.stopStreaming();
    }
};
this.transport.onerror = (event) => {
    this.log(`Transport error: ${event.message}`, 'error'); #logs error
    this.stopStreaming();
};
\end{lstlisting}

The \texttt{await this.transport.ready;} promise ensures that the application waits until the WebTransport connection is established before starting with media capture and stream creation which prevents operations on a disconnected transport. This ensures a stable foundation for subsequent streaming activities.

% Describing media capture and stream initialization
\subsection{Media Capture and Stream Initialization}
Once the WebTransport connection is ready, the client proceeds to capture local media and prepare the data streams for transmission.

\subsubsection{Video/Audio Capture}
Media access is prompted using the \textbf{WebRTC \texttt{getUserMedia} API}, this requests the user for permission to access camera and microphone. The \texttt{constraints} object defines the desired media properties:

\begin{itemize}
    \item \texttt{video}: Configured for resolutions \texttt{640x480} at \texttt{15fps} (Standard Quality) or \texttt{1280x720} at \texttt{30fps} (High Quality), with change of the \texttt{isHighQuality} flag.
    \item \texttt{audio}: Includes \texttt{echoCancellation} and \texttt{noiseSuppression} to improve audio clarity.
\end{itemize}

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
const mediaConstraints = {
    video: {
        //Set video dimensions based on quality mode selection
        width: this.isHighQuality ? 1280 : 640,
        height: this.isHighQuality ? 720 : 480,
        //Configure frame rate for optimal performance
        frameRate: this.isHighQuality ? 30 : 15
    },
    audio: {
        //Apply audio processing enhancements
        echoCancellation: true,
        noiseSuppression: true
    }
};
//Capture user media with specified constraints
this.mediaStream = await navigator.mediaDevices.getUserMedia(mediaConstraints);
//Display captured video in local preview element
this.elements.localVideo.srcObject = this.mediaStream;
\end{lstlisting}

The captured \texttt{mediaStream} is assigned to the \texttt{<video id="localVideo">} element, allowing the user to see their own video feed. A \texttt{Promise} is used to wait for the video element to trigger \texttt{loadeddata}, ensuring the media stream is fully ready before processing.

\subsubsection{Stream Creation}
WebTransport's strength lies in its support for \textbf{multiplexing} via individual streams over a single connection. The client leverages \textbf{unidirectional streams} (\texttt{create\allowbreak{}Bidirectional\allowbreak{}Stream()}) for each distinct data type:

\begin{itemize}
    \item \texttt{this.videoStream}: For outgoing video frames.
    \item \texttt{this.audioStream}: For outgoing audio packets.
    \item \texttt{this.chatStream}: For outgoing chat messages.
    \item \texttt{this.fileStream}: For sending arbitrary file data.
    \item \texttt{this.screenStream}: For sending screen share data.
\end{itemize}

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
#Initialize separate WebTransport streams for different data types
#Video stream for transmitting video frame data
this.videoStream = await this.transport.createUnidirectionalStream();
#Audio stream for transmitting audio packet data  
this.audioStream = await this.transport.createUnidirectionalStream();
#Chat stream for transmitting text messages
this.chatStream = await this.transport.createUnidirectionalStream();
\end{lstlisting}

With separate unidirectional streams, the client sends different types of data independently without head-of-line blocking, enhancing real-time performance and simplifying server-side processing.

% Describing packet construction and sending
\subsection{Packet Construction and Sending}
A custom packet format is implemented to encapsulate various data types for transmission over WebTransport streams, allowing the server to easily identify and process incoming data.

\subsubsection{Packet Format}
Each packet that is sent over a WebTransport stream follows a predefined format consisting of a \textbf{32-byte header} followed by a variable-length \textbf{payload}. The header provides essential metadata:

\begin{itemize}
    \item \textbf{\texttt{track\_id} (16 bytes)}: A string identifier, such as \texttt{'live\_video'}, \texttt{'test\_audio'}, \texttt{'live\_chat'}, used by the receiver to identify the packet's content within its stream and is padded with null bytes if it's shorter than 16 bytes.
    \item \textbf{\texttt{payload\_length} (4 bytes)}: A 32-bit unsigned integer that indicates the exact size of the payload in bytes which is required for correct payload reading.
    \item \textbf{\texttt{track\_type} (12 bytes)}: A string, such as \texttt{'video'}, \texttt{'audio'}, \texttt{'chat'}, \texttt{'file'}, \texttt{'screen'}, classifying the general type of data, padded to 12 bytes.
\end{itemize}

The \texttt{buildPacket()} function assembles this structure:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
buildPacket(trackId, trackType, payload, forceSize = null) {
    let finalPayload = payload;
    if (forceSize && forceSize > payload.length) {
        finalPayload = new Uint8Array(forceSize);
        finalPayload.set(payload, 0);
        for (let i = payload.length; i < forceSize; i++) { #Padding
            finalPayload[i] = i % 256; //Padding pattern
        }
    }

    const header = new ArrayBuffer(32);
    const view = new DataView(header);
    //Set track_id (bytes 0-15)
    new Uint8Array(header, 0, 16).set(this.encoder.encode(trackId.substring(0, 16)));
    //Set payload_length (bytes 16-19)
    view.setUint32(16, finalPayload.length, false); //false for big-endian
    //Set track_type (bytes 20-31)
    new Uint8Array(header, 20, 12).set(this.encoder.encode(trackType.substring(0, 12)));

    const packet = new Uint8Array(header.byteLength + finalPayload.length);
    packet.set(new Uint8Array(header), 0);
    packet.set(finalPayload, header.byteLength);
    this.bytesTransferred += packet.length;
    return packet;
}
\end{lstlisting}

The \texttt{forceSize} parameter allows the client to artificially inflate packet size with padding, serving as a testing feature to analyze the impact of larger packets on network performance and server-side buffer management.

\subsubsection{Sending Logic}
Data is sent using the \texttt{WritableStreamDefaultWriter} associated with each WebTransport stream. The \texttt{loggedWrite()} helper function wraps the \texttt{writer.write()} operation, providing robust error handling and immediate logging feedback:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
//Asynchronous function to send packets with comprehensive error handling
async transmitWithLogging(streamWriter, packetData, dataType) {
    try {
        //Attempt to write packet data to the WebTransport stream
        await streamWriter.write(packetData);
        //Log successful transmission with packet details
        this.log(`Successfully transmitted ${dataType} packet: ${packetData.length} bytes`, 'success');
        return true;
    } catch (transmissionError) {
        //Log detailed error information for debugging
        this.log(`Transmission failed for ${dataType} packet: ${transmissionError.message}`, 'error');
        return false;
    }
}
\end{lstlisting>

This function logs every successful packet transmission, including its type and size, aiding in real-time monitoring. In case of failure, such as stream closed, network issue, an error is logged, ensuring communication issues are immediately visible. The \texttt{writer.releaseLock()} is used after sending test packets to allow re-acquisition for continuous streaming.

% Describing video streaming implementation
\subsection{Video Streaming Implementation}
Video streaming is handled by continuously capturing frames from the local video feed, processing them, and sending them over the dedicated video WebTransport stream. The \texttt{startVideoCapture()} method uses a hidden \texttt{HTMLCanvasElement} as an intermediary to process video frames:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
const canvas = document.createElement('canvas');
const ctx = canvas.getContext('2d');
canvas.width = this.isHighQuality ? 1280 : 640;
canvas.height = this.isHighQuality ? 720 : 480;
this.videoWriter = this.videoStream.getWriter();
\end{lstlisting}

A \texttt{captureLoop()} function recursively calls itself using \texttt{setTimeout} to maintain a consistent frame rate (15 FPS for standard quality, 30 FPS for high quality). The loop includes:

\begin{enumerate}
    \item \textbf{Frame Capture}: \texttt{ctx.drawImage(this.elements.localVideo, 0, 0, canvas.width\allowbreak{}, canvas.height);} draws the current video frame onto the canvas.
    \item \textbf{Encoding}: \texttt{canvas.toDataURL('image/jpeg', quality);} converts the canvas content into a JPEG image data URL, with \texttt{quality} set to 0.7 for standard or 0.9 for high quality.
    \item \textbf{Binary Conversion}: The Base64-encoded image data is converted into a \texttt{Uint8Array} payload.
    \item \textbf{Packetization and Sending}: The \texttt{buildPacket()} method creates a packet with \texttt{track\_id 'live\_video'} and \texttt{track\_type 'video'}, applying a \texttt{forceSize} of 2500 bytes for high-quality streaming to test network handling. The packet is sent via \texttt{this.loggedWrite(this.videoWriter, packet, 'VIDEO')}.
    \item \textbf{Metrics Update}: \texttt{this.videoFrameCount++} increments the counter for UI display.
\end{enumerate}

This approach provides granular control over video encoding and packet size, facilitating experimental analysis of quality settings and packet sizes.

% Describing audio streaming implementation
\subsection{Audio Streaming Implementation}
Audio streaming leverages the \textbf{Web Audio API} for efficient capture and processing of microphone input. The \texttt{startAudioCapture()} method sets up an audio processing pipeline:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
const audioContext = new (window.AudioContext || window.webkitAudioContext)();
await audioContext.resume();
const source = audioContext.createMediaStreamSource(this.mediaStream);
const processor = audioContext.createScriptProcessor(4096, 1, 1);
this.audioWriter = this.audioStream.getWriter();
\end{lstlisting}

\begin{itemize}
    \item \textbf{\texttt{AudioContext}}: Provides the framework for audio processing, with \texttt{audioContext.re\allowbreak{}sume()} ensuring processing starts.
    \item \textbf{\texttt{MediaStreamSource}}: Creates an audio node from the \texttt{mediaStream} obtained via \texttt{getUserMedia}.
    \item \textbf{\texttt{ScriptProcessorNode}}: Uses a buffer size of 4096 samples with one input and output channel for mono audio.
\end{itemize}

In the \texttt{processor.onaudioprocess} event handler:

\begin{enumerate}
    \item \textbf{Audio Data Extraction}: \texttt{event.inputBuffer.getChannelData(0)} retrieves raw floating-point audio samples.
    \item \textbf{PCM Conversion}: Samples are converted to 16-bit PCM format:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
const pcmData = new Int16Array(inputData.length);
for (let i = 0; i < inputData.length; i++) {
    pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
}
const payload = new Uint8Array(pcmData.buffer);
    \end{lstlisting}
    \item \textbf{Packetization and Sending}: The \texttt{pcmData} is used as the payload for a packet with \texttt{track\_id 'live\_audio'} and \texttt{track\_type 'audio'}, with a \texttt{forceSize} of 2200 bytes for testing. The packet is sent via \texttt{this.loggedWrite(this.audioWriter, packet, 'AUDIO')}.
    \item \textbf{Metrics Update}: \texttt{this.audioPacketCount++} increments the counter.
\end{enumerate}

The \texttt{source} and \texttt{processor} nodes are connected to the \texttt{audioContext.destination}, ensuring an active audio processing chain.

% Describing chat messaging implementation
\subsection{Chat Messaging Implementation}
The live chat functionality enables real-time text communication using a dedicated WebTransport stream. The \texttt{sendChatMessage()} method is triggered when the user presses "Enter" or clicks the "Send" button:

\begin{enumerate}
    \item \textbf{Message Retrieval and Validation}: The message is retrieved from \texttt{this.elements\allowbreak{}.chatInput.value} and trimmed, exiting if empty.
    \item \textbf{Data Structuring}: The message is encapsulated in a JSON object:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
const chatData = {
    message_id: Date.now(),
    timestamp: Date.now(),
    user: 'streamer',
    message: message,
    type: 'text'
};
const payload = this.encoder.encode(JSON.stringify(chatData));
    \end{lstlisting}
    \item \textbf{Packetization and Sending}: The JSON object is encoded into a \texttt{Uint8Array} and passed to \texttt{buildPacket()} with \texttt{track\_id 'live\_chat'} and \texttt{track\_type 'chat'}, then sent via \texttt{this.loggedWrite()}.
    \item \textbf{Local Display and Metrics}: The message is appended to \texttt{this.elements.chatMe\allowbreak{}ssages}, scrolled to the bottom, the input field is cleared, and \texttt{this.chatMessageCount++} is incremented.
\end{enumerate}

This dedicated chat stream ensures low-latency communication, critical for interactive applications.




% Describing the router construction process
\section{Router Construction}
This section outlines the design and implementation of a modular and efficient WebTransport Router, built on top of the \texttt{aioquic} library. The router employs an event-driven architecture to handle QUIC and HTTP/3 traffic asynchronously. It incorporates a per-stream buffering mechanism for precise packet reconstruction and a configuration-driven approach for flexible routing and runtime adaptability. Additionally, the system includes a microservice proxy with retry logic for reliable delivery and a metrics subsystem for performance monitoring. These design choices create a robust foundation suitable for both research experimentation and real-world deployment.

% Describing the aioquic library
\subsection{The aioquic Library}
Implementing QUIC and HTTP/3 protocols from scratch means creating your library which is not feasible due to their requirements for encryption, connection management, congestion control, and multiplexing. To focus on application-layer routing logic, the \texttt{aioquic} library was chosen as the foundation.

The \texttt{aioquic} library \cite{aioquic-repo} is a mature, open-source Python \cite{python-docs} library that implements QUIC and HTTP/3 protocols entirely in Python, designed for integration with \texttt{asyncio}. It provides a high-level API, including \texttt{QuicConnectionProtocol} and \texttt{H3Connection}, and employs an event-driven programming model, simplifying development.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
from aioquic.asyncio import serve
from aioquic.asyncio.protocol import QuicConnectionProtocol
from aioquic.quic.configuration import QuicConfiguration
from aioquic.quic.events import QuicEvent, ProtocolNegotiated, ConnectionTerminated
from aioquic.h3.connection import H3Connection
from aioquic.h3.events import H3Event, HeadersReceived, DataReceived, WebTransportStreamDataReceived
\end{lstlisting}

Aioquic abstracts low-level transport and cryptographic concerns, allowing to focus on experimenting with stream parsing and routing logic.

% Describing the event-driven router
\subsection{Event-Driven Router}
The router is implemented as a subclass of \texttt{QuicConnectionProtocol}, leveraging its event-driven capabilities. The central entry point for handling protocol events is the \texttt{quic\_event\_received} method, which dispatches incoming events such as connection negotiation, termination, or HTTP stream updates to appropriate handlers.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class WebTransportRouter(QuicConnectionProtocol):
    def quic_event_received(self, event: QuicEvent) -> None:
        if isinstance(event, ProtocolNegotiated):
            #Handle protocol negotiation
            ...
        elif isinstance(event, ConnectionTerminated):
            #Handle connection termination
            ...
        if self._http is not None:
            for http_event in self._http.handle_event(event):
                self._handle_http_event(http_event)

    def _handle_http_event(self, event: H3Event) -> None:
        #Handles HTTP/3 and WebTransport events
        ...
\end{lstlisting}

Modular methods like \texttt{\_handle\_http\_event} and \texttt{\_handle\_wt\_stream\_data} simplify the main event loop, enhancing code readability and maintainability. This scalable structure aligns with best practices for asynchronous network programming.

% Describing packet buffering and parsing
\subsection{Packet Buffering and Parsing}
Managing data streams over QUIC is challenging due to their continuous byte stream nature, which may include fragmented or concatenated packets. The router maintains a seperate buffer for each QUIC stream stored in a dictionary with key as \texttt{stream\_id}. Incoming data is appended to specific stream buffer, which follows a fixed packet format: a 32-byte header followed by the payload. The header includes a 16-byte track ID, a 4-byte payload length, and a 12-byte track type.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
self._stream_buffers = {}

def _handle_wt_stream_data(self, stream_id: int, data: bytes, stream_ended: bool, _recursive_call: bool = False):
    if stream_id not in self._stream_buffers:
        self._stream_buffers[stream_id] = b""
    self._stream_buffers[stream_id] += data
    buffer = self._stream_buffers[stream_id]

    if len(buffer) >= 32:
        header_info = self._parse_packet_header(buffer[:32])
        if header_info:
            expected_payload_length = header_info['payload_length']
            total_packet_length = 32 + expected_payload_length
            if len(buffer) >= total_packet_length:
                payload = buffer[32:total_packet_length]
                asyncio.create_task(self._process_media_packet(header_info, payload))
                self._stream_buffers[stream_id] = buffer[total_packet_length:]

                if len(self._stream_buffers[stream_id]) > 0:
                    self._handle_wt_stream_data(stream_id, b"", stream_ended, _recursive_call=True)
\end{lstlisting}

This approach ensures accurate reassembly of fragmented packets and sequential processing of multiple packets, mitigating potential buffer mismanagement issues through thorough testing.

% Describing configuration-driven routing
\subsection{Configuration-Driven Routing}
To avoid hardcoding microservice addresses, a flexible configuration system was designed using a YAML file to define services, global parameters, and routing behavior. A custom \texttt{ConfigManager} class parses this file into Python dictionaries and objects. Hot-reloading is implemented by periodically checking the fileâ€™s modification time, allowing configuration updates without interrupting the router or active client sessions.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class ConfigManager:
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.services = {}
        self.global_config = {}
        self.last_modified = 0
        self.load_config()

    def load_config(self):
        with open(self.config_path, 'r') as f:
            config_data = yaml.safe_load(f)
        self.global_config = config_data.get('global', {})
        services_config = config_data.get('services', {})
        self.services = {name: ServiceConfig(cfg) for name, cfg in services_config.items()}
        self.last_modified = os.path.getmtime(self.config_path)

    def should_reload(self) -> bool:
        return os.path.getmtime(self.config_path) > self.last_modified
\end{lstlisting}

This design enhances maintainability and system uptime, allowing administrators to modify service endpoints or routing policies dynamically.

% Describing the proxying layer
\subsection{Proxying Layer}
Once a packet is parsed, it is forwarded to the appropriate microservice, each exposing an HTTP/1.1 endpoint. The router uses \texttt{aiohttp} for non-blocking I/O and concurrent forwarding via asynchronous POST requests. The \texttt{\_forward\_to\_service} method constructs requests using configuration metadata and implements a retry mechanism with exponential backoff to handle temporary service outages.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
async def _forward_to_service(self, service_url: str, payload: bytes, retries: int = 3):
    backoff = 0.5
    for attempt in range(retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(service_url, data=payload) as resp:
                    if resp.status == 200:
                        return await resp.read()
        except Exception as e:
            await asyncio.sleep(backoff)
            backoff *= 2
\end{lstlisting}

This integration layer ensures reliable packet delivery to microservices, even in environments with transient network failures.

% Describing metrics and logging
\subsection{Metrics and Logging}
For performance evaluation and debugging, the router includes a \texttt{MetricsLogger} class that logs metrics such as throughput, stream activity, and memory usage to CSV and structured event logs. The \texttt{psutil} library is used to monitor system resources periodically.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class MetricsLogger:
    def log_metrics(self, router_metrics: dict, performance_data: dict = None):
        #Writes performance and routing metrics to CSV and logs
        ...
\end{lstlisting}

These metrics support real-time monitoring and retrospective analysis in research environments. Additionally, the router supports detailed command-line flags for specifying QUIC parameters, certificate paths, log verbosity, and other operational settings, enabling flexible deployment.

% Summarizing the router implementation
This section detailed the implementation of a robust and modular WebTransport Router. Leveraging \texttt{aioquic} for protocol handling, the system employs an event-driven architecture for asynchronous stream processing. A stream-specific buffering model ensures accurate packet parsing, while configuration-driven routing supports flexible deployment and runtime reconfiguration. The proxying logic enables reliable microservice communication, and a built-in metrics system facilitates monitoring and analysis. Together, these components form a technically sound router ready for research or production environments.





\subsection{The aioquic Library}

Handling the QUIC and HTTP/3 protocols from scratch is prohibitively complex due to their intricacies in encryption, connection management, congestion control, and multiplexing. To avoid reinventing the protocol stack and remain focused on the application-layer routing logic, the \texttt{aioquic} library was selected as the foundation.

\texttt{aioquic} is a mature, open-source Python library that implements QUIC and HTTP/3 entirely in Python and is designed for integration with \texttt{asyncio}. It exposes a high-level API, including \texttt{QuicConnectionProtocol} and \texttt{H3Connection}, and uses an event-driven programming model, significantly simplifying development.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
from aioquic.asyncio import serve
from aioquic.asyncio.protocol import QuicConnectionProtocol
from aioquic.quic.configuration import QuicConfiguration
from aioquic.quic.events import QuicEvent, ProtocolNegotiated, ConnectionTerminated
from aioquic.h3.connection import H3Connection
from aioquic.h3.events import H3Event, HeadersReceived, DataReceived, WebTransportStreamDataReceived
\end{lstlisting}

This library abstracts the low-level transport and cryptographic concerns, allowing the developer to focus on stream parsing and routing logic.

\subsection{Event-Driven Router}

The router was implemented as a subclass of \texttt{QuicConnectionProtocol}, leveraging its event-driven capabilities. The central entry point for handling protocol events is the \texttt{quic\_event\_received} method. Inside this method, incoming events (such as connection negotiation, termination, or HTTP stream updates) are dispatched to appropriate handlers.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class WebTransportRouter(QuicConnectionProtocol):
    def quic_event_received(self, event: QuicEvent) -> None:
        if isinstance(event, ProtocolNegotiated):
            #Handle protocol negotiation
            ...
        elif isinstance(event, ConnectionTerminated):
            #Handle connection termination
            ...
        if self._http is not None:
            for http_event in self._http.handle_event(event):
                self._handle_http_event(http_event)

    def _handle_http_event(self, event: H3Event) -> None:
        #Handles HTTP/3 and WebTransport events
        ...
\end{lstlisting}

Modular methods like \texttt{\_handle\_http\_event} and \texttt{\_handle\_wt\_stream\_data} simplify the main event loop, improving code readability and maintainability. This structure is scalable and aligns with best practices for asynchronous network programming.

\subsection{Packet Buffering and Parsing}

Handling data streams over QUIC presented one of the most technically demanding challenges. Unlike traditional TCP sockets that might send one message per packet, QUIC streams transmit a continuous byte stream, which may contain fragmented or concatenated packets.

To manage this, the router maintains a dedicated buffer for each QUIC stream, using a dictionary keyed by \texttt{stream\_id}. Each incoming data chunk is appended to the stream-specific buffer. The router uses a fixed packet format with 32-byte header following with payload. The header consists of a 16-byte track ID, a 4-byte payload length and a 12-byte track type.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
self._stream_buffers = {}

def _handle_wt_stream_data(self, stream_id: int, data: bytes, stream_ended: bool, _recursive_call: bool = False):
    if stream_id not in self._stream_buffers:
        self._stream_buffers[stream_id] = b""
    self._stream_buffers[stream_id] += data
    buffer = self._stream_buffers[stream_id]

    if len(buffer) >= 32:
        header_info = self._parse_packet_header(buffer[:32])
        if header_info:
            expected_payload_length = header_info['payload_length']
            total_packet_length = 32 + expected_payload_length
            if len(buffer) >= total_packet_length:
                payload = buffer[32:total_packet_length]
                asyncio.create_task(self._process_media_packet(header_info, payload))
                self._stream_buffers[stream_id] = buffer[total_packet_length:]

                if len(self._stream_buffers[stream_id]) > 0:
                    self._handle_wt_stream_data(stream_id, b"", stream_ended, _recursive_call=True)
\end{lstlisting}

This approach ensures fragmented packets are correctly reassembled, and multiple packets arriving in one event are processed in order. Buffer mismanagement could easily cause subtle bugs, making thorough testing essential.

\subsection{Configuration-Driven Routing}

Rather than hardcoding microservice addresses into the codebase, a flexible configuration system was designed. A YAML file defines the available services, global parameters, and routing behavior. This file is read by a custom \texttt{ConfigManager} class, which parses it into Python dictionaries and objects.

Hot-reloading was implemented by periodically checking the fileâ€™s modification time. If the timestamp changes, the configuration is reloaded without interrupting the router or active client sessions.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class ConfigManager:
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.services = {}
        self.global_config = {}
        self.last_modified = 0
        self.load_config()

    def load_config(self):
        with open(self.config_path, 'r') as f:
            config_data = yaml.safe_load(f)
        self.global_config = config_data.get('global', {})
        services_config = config_data.get('services', {})
        self.services = {name: ServiceConfig(cfg) for name, cfg in services_config.items()}
        self.last_modified = os.path.getmtime(self.config_path)

    def should_reload(self) -> bool:
        return os.path.getmtime(self.config_path) > self.last_modified
\end{lstlisting}

This method significantly improves maintainability and system uptime. Administrators can modify service endpoints or routing policies on the fly without restarting the router minimizing deployment overhead.

\subsection{Proxying Layer}

Once a valid packet is extracted and parsed, it must be forwarded to the appropriate microservice. Each service exposes an HTTP/1.1 endpoint. To support non-blocking I/O and concurrent forwarding, the router uses \texttt{aiohttp} to send asynchronous POST requests.

The \texttt{\_forward\_to\_service} method constructs requests using configuration metadata and includes retry mechanisms with exponential backoff strategy to deal with service outages.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
async def _forward_to_service(self, service_url: str, payload: bytes, retries: int = 3):
    backoff = 0.5
    for attempt in range(retries):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(service_url, data=payload) as resp:
                    if resp.status == 200:
                        return await resp.read()
        except Exception as e:
            await asyncio.sleep(backoff)
            backoff *= 2
\end{lstlisting}

This integration layer ensures packets are delivered reliably to microservices, even in environments with transient network failures.

\subsection{Metrics and Logging}

To enable performance evaluation and debugging, the router includes a \texttt{MetricsLogger} class. This component logs metrics such as throughput, stream activity, and memory usage to CSV and structured event logs. The \texttt{psutil} library is used to monitor system resources periodically.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
class MetricsLogger:
    def log_metrics(self, router_metrics: dict, performance_data: dict = None):
        # Writes performance and routing metrics to CSV and logs
        ...
\end{lstlisting}

These metrics are useful for both real-time monitoring and retrospective analysis in research environments.

Additionally, the router supports detailed command-line flags for specifying QUIC parameters, certificate paths, log verbosity, and other operational settings, enabling deployment in diverse scenarios.


This chapter detailed the implementation of a robust and modular WebTransport Router. Starting with the choice of \texttt{aioquic} for protocol handling, the system was built using an event-driven architecture that processed streams asynchronously. A stream-specific buffering model enabled safe and accurate packet parsing. Configuration-driven routing allowed flexible deployment and runtime reconfiguration. Finally, the proxying logic enabled reliable communication with microservices, and a built-in metrics system facilitated monitoring and analysis. Altogether, these components formed a router that is both technically sound and ready for use in research or production environments.




% Describing the microservice and Pulsar producer construction process
\section{Microservice Layer}
This section details the implementation of a generic HTTP-based microservice designed for processing real-time data ingestion and optional forwarding to Apache Pulsar. The microservice handles various data streams like audio, video, chat messages, screen captures, and file uploads, relayed from the WebTransport Router. For this dissertation, only audio video and chat services are handled. The design prioritizes flexibility, scalability, and operational robustness through a modular architecture that supports dynamic configuration, seamless Pulsar integration, and efficient request handling. Each component is described below, including its purpose, implementation, and design rationale.

% Describing the configuration manager
\subsection{Configuration Manager}
The configuration manager loads the microserviceâ€™s runtime settings from a YAML file, enabling operators to modify behavior (such as enabling or disabling Pulsar integration or updating broker URLs) without restarting the service. It supports hot-reloading by detecting file changes, ensuring zero-downtime updates and enhanced operational flexibility.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
import os
import yaml

class MicroserviceConfigManager:
    def __init__(self, config_path):
        self.config_path = config_path
        self.last_modified = 0
        self.config = {}
        self.load_config()

    def load_config(self):
        with open(self.config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        self.last_modified = os.path.getmtime(self.config_path)

    def reload_if_modified(self):
        current_mod_time = os.path.getmtime(self.config_path)
        if current_mod_time > self.last_modified:
            self.load_config()
\end{lstlisting}

Above class reads the YAML configuration and stores its contents in memory. This tracks the fileâ€™s last modified time to detect changes and reloads the configuration when necessary. This design allows critical parameters, such as Pulsar broker URLs or toggle flags, to be updated live, minimizing downtime and avoiding service restarts.

% Describing Apache Pulsar client initialization
\subsection{Apache Pulsar Client Initialization}
This component deals with the connection to the Apache Pulsar broker and produces messages for the microserviceâ€™s specific topic. It supports reinitialization if configuration changes occur, ensuring the service connects to the correct broker with updated settings.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
import pulsar

pulsar_client = None
producer = None

def initialize_pulsar(broker_url, service_name, send_to_pulsar):
    global pulsar_client, producer
    if pulsar_client:
        pulsar_client.close()

    if not send_to_pulsar:
        pulsar_client = None
        producer = None
        return

    pulsar_client = pulsar.Client(broker_url)
    topic = f"persistent://public/default/{service_name}-topic"
    producer = pulsar_client.create_producer(topic, batching_enabled=False)
\end{lstlisting}

The function closes any existing Pulsar client to prevent resource leaks and checks if Pulsar forwarding is enabled. If disabled, it clears connections. Otherwise, it establishes a connection to the specified broker and creates a producer for a topic named after the microservice role. Disabling batching ensures immediate message delivery, critical for real-time data streaming.

% Describing the HTTP data handler
\subsection{HTTP Data Handler}
The HTTP data handler processes incoming HTTP POST requests containing data streams from the WebTransport Router. It extracts the payload and metadata, optionally forwards the data to Pulsar, and responds with a confirmation. The design supports multiple media types with unified logic, enhancing code reuse and simplicity.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
from aiohttp import web

class GenericServiceHandler:
    def __init__(self, service_name):
        self.service_name = service_name
        self.packet_count = 0

    async def process_request(self, request):
        self.packet_count += 1
        payload = await request.read()
        track_id = request.headers.get('X-Track-ID', 'unknown')

        if producer:
            producer.send(payload, properties={"track_id": track_id})

        return web.json_response({
            "status": "success",
            "packet_id": self.packet_count,
            "track_id": track_id,
            "payload_size": len(payload)
        })
\end{lstlisting}

For each request, the handler reads the raw data stream and metadata, such as \texttt{track\_id}, increments a packet counter, and, if the Pulsar producer is active, sends the data with metadata properties for downstream identification. It responds with a JSON object confirming successful processing. This abstraction ensures simplicity and flexibility across different data types.

% Describing service startup and configuration watcher
\subsection{Service Startup and Configuration Watcher}
This component launches an asynchronous HTTP server on a configurable port and exposes a dynamic route based on the service name, such as \texttt{/process\_video}. It continuously monitors the configuration file and reinitializes Pulsar connections if settings change, enabling dynamic adaptability without service restarts.

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
import asyncio
from aiohttp import web

async def start_service(service_name, port, config_path):
    config_manager = MicroserviceConfigManager(config_path)
    send_to_pulsar = config_manager.config.get('send_to_pulsar', False)
    broker_url = config_manager.config.get('pulsar_broker_url', '')

    initialize_pulsar(broker_url, service_name, send_to_pulsar)
    handler = GenericServiceHandler(service_name)

    app = web.Application()
    app.router.add_post(f'/process_{service_name}', handler.process_request)
    runner = web.AppRunner(app)
    await runner.setup()
    site = web.TCPSite(runner, '0.0.0.0', port)
    await site.start()

    while True:
        await asyncio.sleep(10)
        config_manager.reload_if_modified()
        new_send = config_manager.config.get('send_to_pulsar', False)
        new_url = config_manager.config.get('pulsar_broker_url', '')
        if new_send != send_to_pulsar or new_url != broker_url:
            send_to_pulsar = new_send
            broker_url = new_url
            initialize_pulsar(broker_url, service_name, send_to_pulsar)
\end{lstlisting}

This function starts the \texttt{aiohttp} server with a route specific to the service role, loads initial configuration settings, and establishes the Pulsar connection. It periodically checks for configuration changes every 10 seconds, updating Pulsar connection parameters dynamically if needed, preventing service downtime.

% Describing the complete microservice flow
\subsection{Microservice Complete Flow}
At startup, the microservice loads its configuration from a YAML file, defining settings such as the service name, port, Pulsar broker URL, and whether data should be forwarded to Apache Pulsar. Based on these settings, it establishes a Pulsar connection if enabled, allowing real-time data publication to a designated topic. The service exposes an HTTP endpoint named according to its role, such as \texttt{/process\_video}, \texttt{/process\_audio}, or \texttt{/process\_chat}. Upon receiving data through this endpoint, the HTTP handler processes the raw payload and associated metadata, such as a tracking ID, and optionally forwards it to Pulsar for downstream processing. Concurrently, the service monitors the configuration file for changes. If modifications are detected (such as updates to the broker URL or toggling data forwarding), it reloads the configuration and reinitializes the Pulsar connection without restarting. This design yields a reusable codebase capable of handling multiple streaming data types, ensuring flexibility, scalability, and operational robustness across deployments.


\section{Kubernetes Deployment}
This implementation is critical for the completion of the proposed WebTransport solution. The router code is designed to load balance streams of live data to separate microservices. The router, microservice code, and Pulsar components are deployed into a Kubernetes environment within a Minikube cluster. This section details the steps and configurations required for a successful deployment.

% Describing deployment secrets
\subsection{Deployment Secrets}
To enable the router to listen for WebTransport streams, Kubernetes must securely store the necessary certificates. Certificates are stored as secrets within the cluster, allowing resources in the default namespace to access them when mounted on a specified path. The following command was used to create a TLS secret:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
kubectl create secret tls quic-cert --cert=...crt --key=...key
\end{lstlisting}

Above command creates a secret named \texttt{quic-cert} in the default namespace, making the certificate and key accessible to all resources within that namespace.

% Describing router deployment
\subsection{Router Deployment}
The router deployment involves containerization, creating ConfigMaps, defining a deployment, and exposing a service. The routerâ€™s configuration file, as described in the router implementation, supports hot-reloading for dynamic updates.

The router code was containerized using a Dockerfile placed in the same directory as the router code, with the \texttt{router\_config.yaml} file kept external to the image. The following commands built and pushed the Docker image to a DockerHub repository:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
docker build -t sbnm007/quic_router:0.0.0 .
docker push sbnm007/quic_router:0.0.0
\end{lstlisting}

This process successfully created a container image for the router, ready for deployment to the Kubernetes cluster.

The \texttt{router\_config.yaml} file, which defines routing rules for microservices, was deployed as a Kubernetes \texttt{ConfigMap}. This practice enhances extensibility by allowing dynamic addition or removal of microservices through endpoint configuration, avoiding hardcoded values and the need to rebuild the image. The \texttt{router.yaml} file encapsulates the complete Kubernetes configuration for the WebTransport Router, enabling dynamic routing of real-time data, such as video, audio, and chat, to respective microservices. It includes:

\begin{itemize}
    \item A \texttt{ConfigMap} containing \texttt{router\_config.yaml}, supporting hot-reloading of routing rules such as endpoint, port, data format, and custom headers.
    \item A \texttt{Deployment} specifying the Docker image, QUIC/HTTP3 ports, and volume mounts for the configuration file and TLS certificates, configured for a single replica with scalability options.
    \item A \texttt{Secret} for securely injecting TLS certificates to enable encrypted QUIC communication.
    \item Environment variables to locate mounted configuration and certificate files.
    \item A \texttt{Service} of type \texttt{LoadBalancer} exposing the router externally on port 443 for TCP or UDP client connections.
\end{itemize}

The resources were deployed using:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
kubectl apply -f router.yaml
\end{lstlisting}

To verify the deployment, the following commands were used to check resources:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
kubectl get pods
kubectl get svc
\end{lstlisting}

The \texttt{Service} receives an external IP address from MetalLB, enabling external access to the router.

% Describing microservice deployment
\subsection{Microservice Deployment}
The \texttt{microservice.yaml} file defines the Kubernetes configuration for deploying individual microservices such as \texttt{video}, \texttt{audio}, and \texttt{chat} in a scalable and configurable manner. A shared \texttt{ConfigMap} named \texttt{microservice-app-config} contains the \texttt{microservice\_config.yaml} file, mounted into each microservice container at \texttt{/config} and accessible via an environment variable. This enables hot-reloading of settings like the Pulsar broker URL and toggles for Pulsar forwarding.

Each microservice is deployed via a dedicated \texttt{Deployment} resource, running a Python-based service with arguments specifying its type and listening port. A corresponding \texttt{ClusterIP} \texttt{Service} exposes each microservice internally on ports \texttt{4434} for video, \texttt{4435} for audio, and \texttt{4436} for chat, facilitating reliable communication via internal DNS.

The modular architecture allows new microservices to be added by replicating and modifying deployment and service blocks. Key configuration parameters for Pulsar integration include:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
pulsar_broker_url: "pulsar://10.100.48.76:6650"
send_to_pulsar: true
\end{lstlisting}

These values, included in the \texttt{ConfigMap}, support debugging and operational control over Pulsar connectivity.

% Describing Pulsar deployment
\subsection{Pulsar Deployment}
Deploying Apache Pulsar in a constrained Minikube environment presented challenges, particularly with the official Helm chart, which caused out-of-memory (OOM) issues in init containers and failures in BookKeeper nodes. To address this, a lightweight Helm chart from DataStax was used, tailored for development environments. The deployment process involved the following steps:

\begin{itemize}
    \item Add the DataStax Helm chart repository:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
helm repo add datastax-pulsar https://datastax.github.io/pulsar-helm-chart
    \end{lstlisting}
    \item Update the Helm repository:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
helm repo update
    \end{lstlisting}
    \item Download the development configuration:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
curl -LOs https://datastax.github.io/pulsar-helm-chart/examples/dev-values.yaml
    \end{lstlisting}
    \item Modify \texttt{dev-values.yaml} to disable monitoring for reduced resource consumption:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
kube-prometheus-stack:
  enabled: false
    \end{lstlisting}
    \item Deploy Pulsar with the modified configuration:
    \begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
helm install pulsar -f dev-values.yaml datastax-pulsar/pulsar
    \end{lstlisting}
\end{itemize}

This deployment created two key services: \texttt{pulsar-broker} for internal message brokering and \texttt{pulsar-proxy} for external access. The \texttt{pulsar-broker} enables in-cluster communication for microservices, while the \texttt{pulsar-proxy} is exposed externally via a MetalLB load balancer.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/pulsar-svcs.png}
    \caption{Pulsar Kubernetes Services}
    \label{fig:kgetpods}
\end{figure}

A sample Python consumer was developed to demonstrate real-time message consumption, subscribing to a specified topic with a shared subscription type and reporting end-to-end latency:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
consumer = client.subscribe(TOPIC_NAME, SUBSCRIPTION_NAME,
                            consumer_type=pulsar.ConsumerType.Shared)
msg = consumer.receive(timeout_millis=5000)
\end{lstlisting}

This consumer supports real-time consumption of video, audio, or chat streams and is extensible to other topics, validating integration with the Pulsar cluster.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/consumer-video.png}
    \caption{Consumer Connection to Topic}
    \label{fig:consumer-conn}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/pulsar-consumer-video.png}
    \caption{Consumer Receiving Data from Video Topic}
    \label{fig:consumer-data}
\end{figure}

% Describing retrieval of results
\subsection{Retrieving Results}
Metrics are logged in a CSV file within the router pod. To retrieve these files locally for evaluation, the following commands were executed:

\begin{lstlisting}[breaklines=true,basicstyle=\small\ttfamily,frame=single]
mkdir metrics-in-pod
kubectl cp webtransport-router-deployment-65cf75d584-njmqf:/app/metrics_logs ./metrics-in-pod
\end{lstlisting}

This process transfers the CSV files to the local machine, enabling detailed analysis of the systemâ€™s performance metrics.




\section{Network Traffic Analysis}
With both the WebTransport client and router implemented and deployed, this section demonstrates the actual network traffic captured during live streaming sessions. Using Wireshark with the previously configured TLS decryption, we can observe the real-time communication between the client and router, validating the implementation and providing insights into the protocol behavior.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{implementation/network_traffic_analysis.png}
    \caption{Network Traffic Analysis using Wireshark}
    \label{fig:network_traffic_analysis}
\end{figure}


The figure \ref{fig:network_traffic_analysis} shows a snapshot of the network traffic captured on the 'br-5d3c7d61609f' network interface, which is where the custom router is deployed within the Kubernetes cluster. The source IP address 192.168.49.1 represents the WebTransport client, while the destination IP address 192.168.49.200 corresponds to the router's IP address exposed through MetalLB with a custom configured domain name.

The highlighted stream 26 demonstrates the HTTP/3 protocol implementation, with stream number 26 specifically handling live video data. Upon examining the decrypted QUIC packet in the bottom right panel, the packet structure begins with the expected 32-byte header containing the track identifier is correctly set to 'live\_video' as shown in the figure. The subsequent streams represent fragmented packets of the same stream type (live\_video), all maintaining the consistent stream ID 26, which validates the proper stream multiplexing implementation.

This captured traffic successfully validates the implementation of both the WebTransport client and router, demonstrating the successful transmission and processing of live video data. The network capture also reveals audio and chat packets transmitted over stream IDs 30 and 22 respectively, confirming the multi-stream architecture where different media types are isolated into separate streams to prevent head-of-line blocking.





\section{Summary}

In this chapter, a complete end-to-end prototype of the proposed system is implemented and validated within a local Kubernetes environment. The workflow begins with a browser-based WebTransport client that streams multiple types of media, such as video, audio, chat, and files in real time using unidirectional QUIC streams. Each packet is formatted with a custom header and transmitted securely using TLS, with certificate handling and DNS mapping configured for seamless local development. The system demonstrates robust session management, live metrics collection, and a responsive user interface, enabling detailed insight into the streaming behavior.

The WebTransport router, deployed as a scalable microservice, acts as the central intelligence for traffic distribution. It uses asynchronous QUIC and HTTP/3 handling to buffer, parse, and dynamically route packets to the appropriate microservices based on a hot-reloadable YAML configuration. These backend microservices, implemented generically to support various media types, process the data and optionally publish it to Apache Pulsar topics for further stream processing or analytics. The deployment showcases critical infrastructure features such as TLS termination, local DNS resolution, zero-downtime scaling, and runtime observability, all orchestrated through Minikube, MetalLB, and Kubernetes-native tools. This working prototype not only verifies the architectural design but also lays a strong, extensible foundation for production-scale deployments or future research-driven enhancements.